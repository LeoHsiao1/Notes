# AI历史

## 图灵机

- 1936 年，作为数学学士毕业的 Alan Turing ，在英国剑桥大学担任研究员。他发表论文，设想了一种能执行任何数学算法的通用机器。
  - 这种机器被称为图灵机（Turing machine），工作原理如下：
    - 在一条纸带上，记下一个一个符号。
    - 一台机器，依次读取纸带上的符号，然后根据符号执行数学算法。
    - 机器将数学算法的结果，打印在纸带上。
  - 图灵机是一个理论模型，在数学上证明了通用计算机是可以实现的。
    - 如果一个数学问题可以被表达为有限步骤的数学算法，则可以被图灵机解决。
    - 如果一个编程语言能完成图灵机的所有功能，比如算术运算、逻辑运算、条件判断、循环，则称为图灵完备（Turing Completeness）。

- 1945 年，美国的 John von Neumann 在参与研制原子弹的同时，还为 EDVAC（Electronic Discrete Variable Automatic Computer）项目的巨型计算机，设计了一种硬件结构。
  - 他将计算机分为五个基本部分：运算器、控制器、存储器、输入设备、输出设备
  - 这种计算机结构，被称为冯诺依曼结构，后来被大部分计算机采用。

- 1950 年， Alan Turing 发表论文《Computing Machinery and Intelligence》
  - 该论文探讨了一个重要问题： Can machines think?
  - 为了解答该问题，他提出了一个模仿游戏，用于判断一个机器能否思考。这个游戏后来被称为图灵测试，原理如下：
    - 安排一个人类 A 与机器 B ，分别与人类 C 对话。
    - 对话双方，采用写信等间接方式，避免看到对方的模样、透露身份信息。
    - 如果对话之后，人类 C 不能分辨与自己对话的对方，是不是机器。则说明这个机器能模仿人类的思考。

## 人工智能

- 1956 年，美国 Dartmouth 学院的教授 John McCarthy 发起一场关于 thinking machines 的研讨会，邀请了 Claude Shannon、Marvin Minsky、Nathaniel Rochester 等学者。
  - 他们探讨了如何让机器使用人类的语言、模仿人类的思考、代替人类解决问题。
  - 他们首次提出了 AI（Artificial Intelligence，人工智能）的概念，指人类制作的智能。

- 随后几十年，人类研究了许多实现人工智能的技术，主要分为几类：
  - 专家系统
  - 机器学习
  - 符号推理
  - 遗传算法

- AI 技术的常见用途：
  - NLP（Natural Language Processing，自然语言处理）
    - ：让 AI 能理解人类的语言、生成文字。
    - 例如将一篇文章，从英语翻译到汉语。
    - 例如分析一篇文章，提取其内容大意。
  - 计算机视觉
    - ：让 AI 能理解图像、视频。
    - 例如分析一张图片，识别其中存在什么物体。
    - 例如分析一张图片，找出其中的人脸，并进行人脸识别。
  - 数据预测
    - ：让 AI 分析一段数据的历史走势，学习其中的规律，然后预测未来走势。
    - 例如预测股市走势。
    - 例如分析用户的信息，将用户可能感兴趣的广告，推送给用户。
  - AI 决策
    - ：让 AI 对某一问题，给出解决方案。
    - 例如分析病人的信息，进行医疗诊断。
  - AIGC
  - 机器人控制

- 目前，人类已经研发了许多有用的 AI 技术，能让机器代替人类完成许多任务。但人类尚未做出 AGI（Artificial General Intelligence，人工通用智能）。
  - AGI 的一种定义是： AI 能代替人类完成所有任务，具备人类的所有能力。
  - AGI 的另一种定义是： AI 能自主学习，掌握所有能力，不需要为每种能力单独编程。
  - 哲学上，人们还探讨了 strong AI ：不但拥有智慧、能力，还拥有自我意识、情感。

## 专家系统

- 1965 年，美国斯坦福大学的教授 Edward Feigenbaum 带头研发了专家系统（expert system）。
  - 专家系统是一种基于知识库进行决策的软件。知识库内由人类预设了很多 `If... Then...` 形式的规则。
  - 此时， ANN 技术还在初级阶段，难以投入实际应用。而专家系统的研发难度更低，是当时最热门的人工智能技术。

- 1980 年代，专家系统被人们大规模使用。
  - 比如用于医疗领域：如果病人具有某症状，则给出某个诊断结果。
  - 比如用于法律领域：如果一件事符合某法律条文，则给出某个法律结论。
  - 比如一些大学，开设了专家系统的课程。
  - 人们对于专家系统过于乐观，希望它能代替人类进行决策。但随着推广使用，专家系统的缺陷越来越明显：
    - 只能基于呆板的规则进行决策，难以处理意外的情况。
    - 只能进行简单的规则判断，一旦规则变得复杂，就容易出错。

- 1990 年代，研究专家系统的学者越来越少，毕竟 ANN 技术的能力更强、潜力更大。
  - 有些软件依然采用专家系统进行决策，毕竟研发难度比 ANN 技术低很多。
  - 有些软件将专家系统换了个称呼，改名为规则引擎。

## 机器学习

- 1956 年，美国 IBM 公司的 Arthur Samuel ，在 IBM 的第一台商用计算机上，编写了一个能学习下棋的跳棋程序。
  - 当时，他的跳棋程序，已经能战胜业余的人类棋手。这引发了公众轰动，毕竟商用计算机才出现没几年，人们惊叹于计算机的学习能力。
  - Arthur Samuel 写了一个论文，解释这个跳棋程序的原理：
    - 如何让计算机学习下棋？最直接的方案，是让计算机穷举所有可能的棋局，从中找出通往胜利的下棋位置。
    - 但当时的计算机内存很少，因此他采取的方案是：编写一个函数，分析每个下棋位置的获胜概率，从中找出胜率最大的下棋位置。
    - 他还改进了跳棋程序，让程序能记住已发生的棋局位置，保留经验。
    - 他还让程序自己与自己对弈几千场棋，从而积累经验。
  - Arthur Samuel 在论文中，首创了机器学习（Machine Learning）这一概念。
    - 他对机器学习的定义是：使得计算机无需明确编程，就能学习解决问题。
    - 后来，人们改进了机器学习的定义：计算机执行某种任务时，随着经验的增加，能提升某种性能指标。

- 1987 年，美国卡内基梅隆大学的博士生徐峰雄，为国际象棋研发了一台名为 ChipTest 的超级计算机，赢得了北美计算机国际象棋锦标赛的冠军。
  - 徐峰雄毕业之后，加入了 IBM 公司，带队研发了一台名为 Deep Blue 的超级计算机，继续参加国际象棋比赛。
- 1997 年， Deep Blue 在国际象棋比赛中，以2胜1负3平的成绩，战胜了人类的世界冠军 Garry Kasparov 。
  - 这标志着计算机已经能在复杂棋类游戏中战胜人类，引发了公众轰动。
  - Deep Blue 没有采用神经网络，而是由人类手动编写复杂算法、预设大量决策规则。
    - 因此 Deep Blue 能勉强胜任国际象棋，但不能胜任围棋，因为围棋的棋局可能性多很多倍。

- 人类为实现机器学习这一目的，研发了许多种算法，可分为几类：
  - 监督学习
    - 给程序提供一组输入、输出作为示例数据，让程序学习从这些输入映射到输出的一般规律。
    - 通常，需要给程序提供大量数据，让程序找出统计学上的规律。
  - 无监督学习
    - 不提供示例数据，让程序直接学习规律。
  - 半监督学习
    - 只提供少量示例数据。
  - 强化学习
    - 让程序执行某种任务，如果表现好，就给予正反馈作为奖励，从而鼓励程序表现得越来越好。

### 神经网络

- 1943 年，美国的神经科学家 Warren McCulloch 与 Walter Pitts 发表论文，分析了人脑神经元的工作逻辑。
  - 人类大脑的思维过程，是由许多神经元协同工作。每个神经元负责完成一种简单逻辑：如果满足某条件，则做出某决策。
    - 例如，对于 "今天逛公园吗" 这个问题，需要考虑多个条件：
      - 如果喜欢逛公园，则提高意愿一定程度。
      - 如果天气下雨，则降低意愿一定程度。
      - 如果身体不适，则降低意愿一定程度。
  - 可以为一个神经元，建立一个数学模型：
    - 定义多个输入变量，用一个矢量 `X = (x1, x2, x3, ...)` 表示。
    - 每个输入变量，存在不同的权重，用一个矢量 `Y = (y1, y2, y3, ...)` 表示。
    - 定义一个阈值 threshold ，它决定了输出变量 output 的取值：
      - 如果 `X*Y ≤ threshold` ，则 `output = 0` ，决策为否
      - 如果 `X*Y > threshold` ，则 `output = 1` ，决策为是。
    - 只要给矢量 Y 、阈值 threshold 分配合适的值，就能让这个数学模型，模拟人类的决策逻辑。
  - 受此启发，他们提出了一种软件方案：
    - 编写许多个软件单元，每个单元像一个神经元，负责处理一种逻辑判断。
    - 这些软件单元可以协同工作，模拟人脑的神经网络。
    - 只要有足够多的软件单元，这种软件可以执行任何数学算法，实现图灵机。

- 后来，人们继续研究这种模拟神经网络的软件方案，称为人工神经网络（Artificial Neural Network，ANN），简称为 NN 。
  - 编程实现神经网络时，主要逻辑如下：
    - 在代码中存储了权重矢量 Y 、阈值 threshold 的取值。
    - 给代码输入矢量 X 。
    - 代码会自动输出变量 output 。
  - 权重矢量 Y 、阈值 threshold 的取值，是算法模型中的核心参数。如何确定它们的取值？
    - 最初，人们手动赋值，然后测试效果。如果效果不好，就换个取值再测试。
    - 后来，人们编写程序来自动赋值、自动测试。这个过程称为训练模型。
  - 可以将一个神经单元的输出，用作另一个神经单元的输入。以此类推，可以组成 n 层神经网络，实现复杂的决策逻辑。
    - 多层神经网络，还可以采用环形结构，让信号循环传递到各个神经元。这种结构称为 RNN（Recurrent Neural Network）。
    - 用多层神经网络来实现机器学习的方案，称为深度学习（Deep Learning）。

- 1958 年，美国的心理学家 Frank Rosenblatt 发明了感知器（Perceptron）。
  - 它是一个三层的神经网络，包含输入层、隐含层、输出层。
  - 它能检查输出值，如果与预期值不一致，则自动调整权重参数。
  - 它是世界上第一个能自动训练的 ANN 模型，能自己学习，不需要人类手动分配参数。

- 1965 年，苏联的数学家 Alexey Ivakhnenko 发明了 GMDH（Group Method of Data Handling） 算法。
  - 它是第一个可行的深度学习算法，能训练任意 n 层神经网络。
  - 它能根据经验数据，自动确定模型的结构和参数，实现归纳学习。

- 1969 年，日本的福岛邦彦发明了 ReLU（Rectified Linear Unit，整流线性单元），它成为了深度学习中常用的激活函数。
- 1979 年，福岛邦彦发明了第一个带有卷积层的神经网络，开创了卷积神经网络（Convolutional Neural Network，CNN）这一架构。
  - 福岛邦彦的研究领域是图像识别，他从人脑的视觉细胞中得到了启发：存在一种处理简单任务的细胞、一种处理复杂任务的细胞，两种细胞级联工作。

- 1982 年，美国的 Paul Werbos 使用误差反向传播算法，成功训练了多层神经网络。
  - 感知器等更早的算法，只能训练单层神经网络。如果训练多层神经网络，则不能有效地调整多个层的权重。
  - 反向传播算法，能计算每一层的梯度（误差），并将误差从输出层逐层传到输入层，从而逐层调整权重，因此能训练多层神经网络。

- 1989 年，美国贝尔实验室的 Yann LeCun 等人，创建了一个名为 LeNet 的 CNN ，用于识别邮件上的手写邮编。

- 1991 年，德国的 Sepp Hochreiter 在毕业论文中，发现了梯度消失问题。
  - 当时，反向传播算法虽然可以训练多层神经网络，但效果并不好。他找到了问题的原因：梯度传播多层时，较早权重的梯度，会在运算中被缩小幅度。

- 1995 年， Sepp Hochreiter 发明了 LSTM（Long Short Term Memory，长短期记忆）技术，可以解决 RNN 网络的梯度消失问题。
  - LSTM 可以为 RNN 提供可持续数千个时间步的记忆。

- 2012 年，加拿大的 Alex Krizhevsky 及其博士生导师 Geoffrey Hinton ，研发了一个 CNN 网络，名为 AlexNet ，在 ImageNet 图像分类比赛中取得了显著优势。
  - 他们使用两张 Nvidia GPU 来训练模型，比起使用 CPU 来训练模型，耗时大幅缩短。
  - 这引发越来越多人，使用 GPU 来加速训练模型。

- 2016 年，英国 DeepMind 公司被 Google 收购，研发了 AlphaGo 程序，在围棋比赛中，战胜了人类的世界冠军李世石。
  - AlphaGo 采用深度学习，因此不必像 Deep Blue 那样由人类手动编写大量决策规则。
  - AlphaGo 进行了大量训练，包括与人类对弈、与计算机对弈，从而逐渐提高了下棋实力。

## NLP

### Transformer

- 2011 年， Apple 公司发布一款名为 Siri 的对话机器人，安装在 iPhone 等电子产品中。
  - 此时 iPhone 这种触屏手机刚发布 4 年， Siri 实在是一个新潮的技术，是许多人首次接触的对话机器人。
  - 此时 AI 技术跟 20 世纪差不多，只是互联网技术大幅发展，使得 Siri 可以联网搜索资料。
  - Siri 的缺点很多：
    - 它的语音识别能力差，如果用户说话不够清晰、带口音，它就不能识别为文本。
    - 它的语言处理能力差，即使识别了用户说的一句话是什么，也经常误解用户的意图。
    - 它几乎没有逻辑推理能力，只能僵硬地执行用户的命令，比如拨打电话、地图导航。

- 2014 年，微软公司发布一款名为小冰的对话机器人，以网页形式供用户访问。
  - 它基于深度学习的 RNN + LSTM 技术，使用大规模语料库进行训练，能与人类进行日常对话。但不能进行长对话。
  - 它能记住用户习惯等上下文信息，据此调整回答。但只能记住少量上下文信息。
  - 它能分析用户说话的语气、情感，据此调整回答。但只能进行初步的分析。

- 2017 年， Google 公司的几位研究员发布论文《Attention is all you need》，提出一种新的深度学习架构，名为 Transformer 。
  - 当时，对于序列类型的数据（比如文本、音频），业界通常采用 RNN + LSTM 技术来处理。
    - 缺点：能记住一定长度的上下文信息，但超过长度就记不住，因此不能处理较长的序列。
    - 缺点：需要按顺序读取一个序列中的每个 token ，耗时久。
  - 谷歌翻译的 seq2seq 模型也采用 RNN + LSTM 技术。为了解决上述缺点，研究员做了以下改进：
    - 引入注意力机制，代替 LSTM ，使得模型不必记住全部上下文信息，只需记住有用部分。因此模型可以处理较长的序列。
    - 移除循环，并行化处理 token ，从而能在 GPU 上加速执行。

- 2018 年， Google 公司发布一个名为 BERT 的语言模型。
  - 它采用 Transformer 架构。
  - 它进行了生成式预训练（Generative Pre-training，GP），这是一种半监督学习：
    - 先用一个未标记的、所有领域的数据集（比如 Wikipedia 等互联网资料），训练模型的通用能力。
    - 再用一个标记的、单个领域的数据集，进一步微调模型在该领域的能力。
  - 它使用两种方法来预训练：
    - 掩码标记预测：将句子中随机几个单词掩盖，让模型猜测这几个单词是什么。
    - 下一句预测：输入两个句子，比如 "I have a car." 与 "It is small." ，让模型判断这两个句子在语意上是否连续。
  - 传统的语言模型，只能按单个方向分析一个序列中的每个 token 。而 BERT 能顺序、倒序同时分析序列，建立两个方向的上下文。

### GPT

- 2018 年，美国的初创公司 OpenAI 发布一个名为 GPT（Generative Pre-trained Transformer）的语言模型。
  - 它采用 Transformer 架构，进行了预训练+监督微调。
  - 它的模型参数有 1 亿个。
  - 它主要用于生成文本，但也可以生成图像、音频，只是没有经过针对性训练。
  - 缺点：它生成的文本依然比较呆板，容易出现逻辑错误、内容重复。

- 2019 年， OpenAI 公司发布 GPT 的 v2 版本。
  - 与上一代 GPT 相比，它的训练数据集大了几倍，模型参数增加到 15 亿个。
  - 缺点：它生成的文本比较流畅，但依然容易逻辑错误、忘记上下文信息。

- 2020 年， OpenAI 公司发布 GPT 的 v3 版本。
  - 与上一代 GPT 相比，它的训练数据集大了几倍，模型参数增加到 1750 亿个。
  - 它生成的文本很流畅，能处理复杂的长文本，偶尔才出现逻辑错误。换句话说，它处理自然语言的能力逼近真人，在许多情况下可以通过图灵测试。
  - 不过，它对现实世界的理解容易出错：
    - 回答用户的问题时，可能编造不存在的历史事实。这个问题，被人们称为模型的幻觉。
    - 在用户的诱导下，可能说出明显违背事实的错误言论。

- 2022 年 11 月 30 日， OpenAI 公司发布一款名为 ChatGPT 的对话机器人。
  - ChatGPT 提供了网页版对话框，供用户使用。也提供了 API 。
  - ChatGPT 最初采用 GPT-3.5 模型，后来用户可选用 GPT-4 等其它模型。
  - ChatGPT 发布之后，迅速火爆世界， 2 个月时间就拥有 1 亿用户。公众惊讶 AI 技术已经这么强大。
    - 之前的 Siri 等对话机器人，只能进行简单的日常对话。但 ChatGPT 能进行复杂的长对话，几乎像人类。
    - ChatGPT 能理解文本中的语气、情感，甚至能讲幽默笑话。
    - 用户可以问任何领域的专业问题， ChatGPT 都能给出解答（因为用 Wikipedia 等数据训练了模型）。它像一个什么都懂的网友，只是回答不一定完全正确。
    - 许多学生，用 ChatGPT 解答自己的学校作业、考试试卷。
    - 在编程这个专业领域， ChatGPT 能理解一段代码的功能逻辑，进行讲解。能根据用户的需求，生成一段源代码。（因为用 Stack Overflow 等数据训练了模型）
  - ChatGPT 也让许多 AI 学者感到惊讶。
    - 在此之前，对话机器人虽然能与人类日常对话，但不能理解人类世界的常识、不能进行逻辑推理。
    - 因此，许多学者认为，等研究出 AGI 之后，才能让 AI 掌握常识、逻辑。
    - 但 ChatGPT 证明，如果模型训练了海量参数，就能记住很多常识、逻辑，能在许多场合模拟人类的智慧。这种模型被称为 LLM（Large Language Model）。

- 2023 年， OpenAI 公司发布 GPT 的 v4 版本。
  - 它的模型参数超过 1 万亿个。
  - 它增强了多模态能力，能处理多种类型的数据（比如文字生成、图像生成、音频生成），不必为每种数据分别训练一个模型。
  - 它对涉及安全、道德的的内容，进行了更多的对抗训练、内容审查，使得模型很少生成带偏见、有害的内容。

- 2025 年， OpenAI 公司发布 GPT 的 v5 版本。
  - 它带有一个路由器，会根据用户的提问难度，自动选择 GPT-5-main、GPT-5-thinking、GPT-5-mini 等模型来回答。

### LLM

- ChatGPT 火爆世界之后，证明了 LLM 的巨大潜力。许多公司急忙跟着研发 LLM 模型，进行技术竞赛。

- Anthropic 公司
  - 2021 年， OpenAI 的几名员工离职，创立了 Anthropic 公司。
  - 2023 年 3 月，发布一款名为 Claude 的模型。这个名字是致敬信息论的奠基人 Claude Shannon 。

- 微软公司
  - 2019 年，向 OpenAI 公司投资 10 亿美元，约定将 OpenAI 公司的模型部署到 Azure 云平台。
  - 2020 年，为 OpenAI 公司定制了一台超级计算机，拥有 1 万张显卡。这提高了 OpenAI 训练模型的速度。
  - 2023 年，追加 100 亿美元投资，获得 OpenAI 的 49% 股权。将 GPT-3 的 API 整合到微软公司的产品中，命名为 Copilot 。
  - 2024 年，与 OpenAI 公司的合作减少，开始自研模型。
  - 2025 年，发布自研模型 MAI 。

- Google 公司
  - 2020 年，发布一款名为 Meena 的对话机器人，模型参数有 26 亿个。
    - Google 公司限制了 Meena 被公众使用，因为担心对话机器人的能力太强，可能被用户滥用，比如网络诈骗。
  - 2021 年，将 Meena 的模型改名为 LaMDA （Language Model for Dialogue Applications），模型参数有 1370 亿个。
  - 2022 年，发布 LaMDA 的 v2 版本。
  - 2023 年 2 月，发布一款名为 Bard 的对话机器人，基于 LaMDA 模型。
    - Bard 是匆忙发布的产品，在发布会上展示的效果较差，导致 Google 公司的股价下跌 10% 。
  - 2023 年 12 月，发布一款名为 Gemini 的模型，它继承 LaMDA 但做了大幅修改。

- Meta 公司
  - 2023 年 2 月，发布一款名为 Llama 的模型。并开源了 llama.cpp ，作为一个 LLM 运行框架。

- xAI 公司
  - 2023 年 3 月，马斯克创立 xAI 公司，加入 LLM 赛道。
  - 2023 年 11 月，发布名为 Grok 的自研模型，并以对话机器人的形式，集成到 X 网站上，供网络用户免费使用。

- DeepSeek 公司
  - 2016 年，梁文锋在中国创立 High-Flyer 公司，利用 AI 技术进行量化交易。
  - 2023 年， High-Flyer 公司将 AI 部门拆分为独立公司 DeepSeek 。
  - 2023 年 11 月，发布一款名为 DeepSeek-Coder 的模型，其能力较差，还在追赶 ChatGPT 的技术。
  - 2024 年，先后发布 DeepSeek-V2 和 DeepSeek-V3 。
  - 2025 年 1 月，发布 DeepSeek-R1 模型，能力接近 GPT-4 。
    - 它在预训练之后，没有监督微调，而是单纯进行强化学习，从而提升模型的推理能力。
    - 它利用知识蒸馏技术，将大型模型的知识、推理能力，传递给小型模型。
    - 它在网络上公开了模型文件，用户可以下载之后自己部署，而且部署成本低。
      - 最小的模型只有 15 亿参数，简称为 1.5B 。在 8G 内存的个人电脑上就可以部署，甚至可以用 CPU 运行，不必用 GPU 。
      - 大模型有 70B 参数，部署时需要几张显卡、几百GB内存。
    - 它以网页形式，提供了一个免费的 DeepSeek 对话机器人，吸引了许多中文用户。
      - 它支持在网页上显示模型的推理过程，方便用户理解模型的思维方式。

- 2023 年，美国加州大学伯克利分校的博士生团队，发表论文，分析了 LLM 的显存浪费问题、优化方案。
  - LLM 的显存浪费问题：
    - LLM 每次执行一个推理任务时，会在 GPU 显存中暂时缓存大量 key-value cache ，根据这些 cache 生成下一个 token 。
    - 这些 KV cache 的大小会动态变化，难以预测。因此经常产生一些显存碎片，浪费一些显存。
    - 多个任务的 KV cache 相互独立，可能缓存相同的内容，导致重复占用一些显存。
  - 他们借鉴操作系统的虚拟内存机制，发明了 PagedAttention 算法，来提高显存的利用率。
    - 它以 page 为单位，存储 KV Cache 。
    - 每个 page 的大小固定，因此减少了内部碎片。
    - 每个 page 可以映射到显存中的不连续物理地址，因此减少了外部碎片。
    - 多个任务，如果缓存了相同的数据，则它们的 page 会映射到显存中的同一物理地址。
  - 他们开源了 vLLM 项目，基于 PagedAttention 算法来运行 LLM 。

### AIGC

- AIGC（AI Generated Content）
  - ：让 AI 生成内容，比如文本、图像、音频、视频。
  - 它又称为 Generative AI、GAI ，是 AI 技术的一个常见用途。
  - 优点：
    - 让 AI 代替人类完成一些编辑工作。某些场景下，比人类更快、更好。
  - 缺点：
    - AI 是根据现有资料来生成内容，因此擅长生成重复性内容，不擅长生成新颖内容。
      - 虽然增加随机程度，可以生成比较新颖的内容，但是结果会比较混乱，比如物体破损、图案扭曲。需要尝试生成几十张图像，才能得到一张没有明显瑕疵的图像。
    - AI 是根据现有资料来生成内容。如果现有资料的版权属于某个人，那么 AIGC 的内容，版权属于谁？算抄袭吗？这存在法律问题。

- 21 世纪初， AIGC 技术还在起步阶段，只能在少量场景，执行简单任务。
  - 例如进行图像风格的转换：基于 CNN 技术，先对图像内容进行语义分割，再通过线条弯曲、风格迁移等手法，将图片内容转换成另一种艺术风格，实现修图软件的滤镜效果。

- 2020 年代， AIGC 技术爆发式发展。人们研发了多种深度学习模型来执行 AIGC 任务，这些模型统称为 Generative Model ，如下：
  - VAE（Variational Auto Encoder，变分自编码器）
    - 2006 年，提出一种名为 Autoencoder 的无监督学习模型，它分为两个部分：编码器（Encoder）、解码器（Decoder）。
    - 2013 年，在 Autoencoder 的基础上，加上变分推理，组成 VAE 架构。
      - 它基于概率对数据进行编码，擅长进行数据压缩、图像降噪。
  - Diffusion Model（扩散模型）
    - 2015 年，一篇论文中探讨了参考热力学扩散过程，进行无监督学习的方法。后来发展成为 Diffusion Model 。
    - 原理：
      - 正向扩散：输入一张图像，逐步添加随机噪声，将它转换成一张马赛克图像。
      - 反向扩散：然后让模型去掉噪声，学习如何将马赛克图像，转换成正常的图像。
  - GAN（Generative Adversarial Network，生成式对抗网络）
    - 2014 年， Google 公司的 Ian Goodfellow 发明了 GAN 技术。
    - 原理：
      - 使用两个神经网络来对抗。一个神经网络负责生成图像，另一个神经网络负责检测该图像是真是假。
      - 多次重复对抗过程，两个神经网络都会不断学习进步，从而提高生成的图像质量。
  - Transformer

- 2021 年， OpenAI 公司发布一款名为 DALL-E 的模型。
  - 它允许用户输入一段描述文本，然后由模型生成相应的图像。
  - 原理：使用 GPT-3 生成一段 token 序列，然后用 VAE 将 token 序列转换为图像。

- 2022 年，英国公司 Stability AI 发布一款名为 Stable Diffusion 的模型。
  - 它的 text-to-image 能力更优秀，可以生成很逼真的图像。
  - 它使用潜在扩散（Latent Diffusion）技术。
    - Diffusion Model 是直接对图像进行扩散，需要处理所有像素点，运算耗时久。
    - 而 Latent Diffusion 是先用 VAE 将像素空间，映射到低维的潜在空间（Latent Space），然后进行扩散，从而大幅减少运算量。
      - 比如将一张 1024x1024 分辨率的图片，有损压缩 64×64 分辨率。进行扩散之后，再放大到 1024x1024 分辨率。
      - 缺点是，有损压缩容易丢失细节，比如图片中的文字容易扭曲。
