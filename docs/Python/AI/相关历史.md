# 相关历史

## 图灵机

- 1936 年，作为数学学士毕业的 Alan Turing ，在英国剑桥大学担任研究员。他发表论文，设想了一种能执行任何数学算法的通用机器。
  - 这种机器被称为图灵机（Turing machine），工作原理如下：
    - 在一条纸带上，记下一个一个符号。
    - 一台机器，依次读取纸带上的符号，然后根据符号执行数学算法。
    - 机器将数学算法的结果，打印在纸带上。
  - 图灵机是一个理论模型，在数学上证明了通用计算机是可以实现的。
    - 如果一个数学问题可以被表达为有限步骤的数学算法，则可以被图灵机解决。
    - 如果一个编程语言能完成图灵机的工作，比如算术运算、逻辑运算、条件判断、循环，则称为图灵完备（Turing Completeness）。

- 1945 年，美国的 John von Neumann 在参与研制原子弹的同时，还为 EDVAC（Electronic Discrete Variable Automatic Computer）项目的巨型计算机，设计了一种硬件结构。
  - 他将计算机分为五个基本部分：运算器、控制器、存储器、输入设备、输出设备
  - 这种计算机结构，被称为冯诺依曼结构，后来被大部分计算机采用。

- 1950 年， Alan Turing 发表论文《Computing Machinery and Intelligence》
  - 该论文探讨了一个重要问题： Can machines think?
  - 为了解答该问题，他提出了一个模仿游戏，用于判断一个机器能否思考。这个游戏后来被称为图灵测试，原理如下：
    - 安排一个人类 A 与机器 B ，分别与人类 C 对话。
    - 对话双方，采用写信等间接方式，避免看到对方的模样、透露身份信息。
    - 如果对话之后，人类 C 不能分辨与自己聊天的对方，是不是机器。则说明这个机器能模仿人类的思考。

## 人工智能

- 1956 年，美国 Dartmouth 学院的教授 John McCarthy 发起一场关于 thinking machines 的研讨会，邀请了 Claude Shannon、Marvin Minsky、Nathaniel Rochester 等学者。
  - 他们探讨了如何让机器使用人类的语言、模仿人类的思考、代替人类解决问题。
  - 他们首次提出了人工智能（Artificial Intelligence）的概念。

- 随后几十年，人类研究了许多实现人工智能的技术，主要分为几类：
  - 专家系统
  - 机器学习
  - 符号推理
  - 遗传算法

## 专家系统

- 1965 年，美国斯坦福大学的教授 Edward Feigenbaum 带头研发了专家系统（expert system）。
  - 专家系统是一种基于知识库进行决策的软件。知识库内由人类预设了很多 `If... Then...` 形式的规则。
  - 此时， ANN 技术还在初级阶段，难以投入实际应用。而专家系统的研发难度更低，是当时最热门的人工智能技术。

- 1980 年代，专家系统被人们大规模使用。
  - 比如用于医疗领域：如果病人具有某症状，则给出某个诊断结果。
  - 比如用于法律领域：如果一件事符合某法律条文，则给出某个法律结论。
  - 比如一些大学，开设了专家系统的课程。
  - 人们对于专家系统过于乐观，希望它能代替人类进行决策。但随着推广使用，专家系统的缺陷越来越明显：
    - 只能基于呆板的规则进行决策，难以处理意外的情况。
    - 只能进行简单的规则判断，一旦规则变得复杂，就容易出错。

- 1990 年代，研究专家系统的学者越来越少，毕竟 ANN 技术的能力更强、潜力更大。
  - 有些软件依然采用专家系统进行决策，毕竟研发难度比 ANN 技术低很多。
  - 有些软件将专家系统换了个称呼，改名为规则引擎。

## 机器学习

- 1956 年，美国 IBM 公司的 Arthur Samuel ，在 IBM 的第一台商用计算机上，编写了一个能学习下棋的跳棋程序。
  - 当时，他的跳棋程序，已经能战胜业余的人类棋手。这引发了公众轰动，毕竟商用计算机才出现没几年，人们惊叹于计算机的学习能力。
  - Arthur Samuel 写了一个论文，解释这个跳棋程序的原理：
    - 如何让计算机学习下棋？最直接的方案，是让计算机穷举所有可能的棋局，从中找出通往胜利的下棋位置。
    - 但当时的计算机内存很少，因此他采取的方案是：编写一个函数，分析每个下棋位置的获胜概率，从中找出胜率最大的下棋位置。
    - 他还改进了跳棋程序，让程序能记住已发生的棋局位置，保留经验。
    - 他还让程序自己与自己对弈几千场棋，从而积累经验。
  - Arthur Samuel 在论文中，首创了机器学习（Machine Learning）这一概念。
    - 他对机器学习的定义是：使得计算机无需明确编程，就能学习解决问题。
    - 后来，人们改进了机器学习的定义：计算机执行某种任务时，随着经验的增加，能提升某种性能指标。

- 1987 年，美国卡内基梅隆大学的博士生徐峰雄，为国际象棋研发了一台名为 ChipTest 的超级计算机，赢得了北美计算机国际象棋锦标赛的冠军。
  - 徐峰雄毕业之后，加入了 IBM 公司，带队研发了一台名为 Deep Blue 的超级计算机，继续参加国际象棋比赛。
- 1997 年， Deep Blue 在国际象棋比赛中，以2胜1负3平的成绩，战胜了人类的世界冠军 Garry Kasparov 。
  - 这标志着计算机已经能在复杂棋类游戏中战胜人类，引发了公众轰动。
  - Deep Blue 没有采用神经网络，而是由人类手动编写复杂算法、预设大量决策规则。
    - 因此 Deep Blue 能勉强胜任国际象棋，但不能胜任围棋，因为围棋的棋局可能性多很多倍。

- 人类为实现机器学习这一目的，研发了许多种算法，可分为几类：
  - 监督学习
    - 给程序提供一组输入、输出作为示例数据，让程序学习从这些输入映射到输出的一般规律。
    - 通常，需要给程序提供大量数据，让程序找出统计学上的规律。
  - 无监督学习
    - 不提供示例数据，让程序直接学习规律。
  - 半监督学习
    - 只提供少量示例数据。
  - 强化学习
    - 让程序执行某种任务，如果表现好，就给予正反馈作为奖励，从而鼓励程序表现得越来越好。

### 神经网络

- 1943 年，美国的神经科学家 Warren McCulloch 与 Walter Pitts 发表论文，分析了人脑神经元的工作逻辑。
  - 人类大脑的思维过程，是由许多神经元协同工作。每个神经元负责完成一种简单逻辑：如果满足某条件，则做出某决策。
    - 例如，对于 "今天逛公园吗" 这个问题，需要考虑多个条件：
      - 如果喜欢逛公园，则提高意愿一定程度。
      - 如果天气下雨，则降低意愿一定程度。
      - 如果身体不适，则降低意愿一定程度。
  - 可以为一个神经元，建立一个数学模型：
    - 定义多个输入变量，用一个矢量 `X = (x1, x2, x3, ...)` 表示。
    - 每个输入变量，存在不同的权重，用一个矢量 `Y = (y1, y2, y3, ...)` 表示。
    - 定义一个阈值 threshold ，它决定了输出变量 output 的取值：
      - 如果 `X*Y ≤ threshold` ，则 `output = 0` ，决策为否
      - 如果 `X*Y > threshold` ，则 `output = 1` ，决策为是。
    - 只要给矢量 Y 、阈值 threshold 分配合适的值，就能让这个数学模型，模拟人类的决策逻辑。
  - 受此启发，他们提出了一种实现图灵机的软件方案：编写许多个软件单元，每个单元负责处理一种逻辑判断。
    - 这些软件单元协同工作，就可以模拟人脑的神经网络。
    - 这种方案，称为 ANN（Artificial Neural Network，人工神经网络），简称为 ANN（Neural Network）。
  - 可以将一个神经单元的输出，用作另一个神经单元的输入。以此类推，可以组成 n 层神经网络，实现更复杂的逻辑。
    - 用多层神经网络来实现机器学习的方案，称为深度学习（Deep Learning）。
    - 多层神经网络，还可以采用环形结构，让信号循环传递到各个神经元。这种结构的 ANN ，称为 RNN（Recurrent Neural Network）。

- 1958 年，美国的心理学家 Frank Rosenblatt 发明了感知器（Perceptron）。
  - 它是一个三层的神经网络，包含输入层、隐含层、输出层。
  - 它是第一个能通过反复试验，学习新技能的计算机程序。

- 1965 年，苏联的数学家 Alexey Ivakhnenko 发明了 GMDH（Group Method of Data Handling） 算法。
  - 它是第一个可行的深度学习算法，能训练任意 n 层神经网络。
  - 它能根据经验数据，自动确定模型的结构和参数，实现归纳学习。

- 1969 年，日本的福岛邦彦发明了 ReLU（Rectified Linear Unit，整流线性单元），它成为了深度学习中常用的激活函数。
- 1979 年，福岛邦彦发明了第一个带有卷积层的神经网络，开创了卷积神经网络（Convolutional Neural Network，CNN）这一架构。
  - 福岛邦彦的研究领域是图像识别，他从人脑的视觉细胞中得到了启发：存在一种处理简单任务的细胞、一种处理复杂任务的细胞，两种细胞级联工作。

- 1982 年，美国的 Paul Werbos 使用误差反向传播算法，成功训练了多层神经网络。
  - 感知器等更早的算法，只能训练单层神经网络。如果训练多层神经网络，则不能有效地调整多个层的权重。
  - 反向传播算法，能计算每一层的梯度（误差），并将误差从输出层逐层传到输入层，从而逐层调整权重，因此能训练多层神经网络。

- 1989 年，美国贝尔实验室的 Yann LeCun 等人，创建了一个名为 LeNet 的 CNN ，用于识别邮件上的手写邮编。

- 1991 年，德国的 Sepp Hochreiter 在毕业论文中，发现了梯度消失问题。
  - 当时，反向传播算法虽然可以训练多层神经网络，但效果并不好。他找到了问题的原因：梯度传播多层时，较早权重的梯度，会在运算中被缩小幅度。

- 1995 年， Sepp Hochreiter 发明了 LSTM（Long Short Term Memory，长短期记忆）技术，可以解决 RNN 网络的梯度消失问题。
  - LSTM 可以为 RNN 提供可持续数千个时间步的记忆。

- 2012 年，加拿大的 Alex Krizhevsky 及其博士生导师 Geoffrey Hinton ，研发了一个 CNN 网络，名为 AlexNet ，在 ImageNet 图像分类比赛中取得了显著优势。
  - 他们使用两张 Nvidia GPU 来训练模型，比起使用 CPU 来训练模型，耗时大幅缩短。
  - 这引发越来越多人，使用 GPU 来加速训练模型。

- 2016 年，英国 DeepMind 公司的研发的 AlphaGo 程序，在围棋比赛中，战胜了人类的世界冠军李世石。
  - AlphaGo 采用深度学习，因此不必像 Deep Blue 那样由人类手动编写大量决策规则。
  - AlphaGo 进行了大量训练，包括与人类对弈、与计算机对弈，从而逐渐提高了下棋实力。

## AIGC

AI 内容生成（AI Generated Content，AIGC）技术：泛指使用 AI 自动生成文字、图像、音频、视频、代码等内容，代替人工。


AI 图像生成技术的发展阶段：
- 2015 年，主流的技术是通过基于卷积神经网络的迁移学习，来进行图像风格转换。先对图像内容进行语义分割，再将内容和场景通过线条弯曲、风格迁移等手法，转换成指定艺术风格，类似美颜软件的“滤镜”功能。
- 2016 年，流行的技术是生成对抗网络 GAN（Generative Adversarial Nets）。原理：让生成器网络和判别器网络相互对抗，从而创作出真实度和准确度都更高的全新图像。此时网络上出现了大量“以假乱真”的艺术作品。
  - 但是，GAN也不能摆脱传统AI深度模型的问题：无法理解“逻辑”和“常识”，比如AI能够根据文本关键词把元素堆叠在一起，但因为无法理解隐藏在自然语言背后的逻辑关系，所以经常会画出非常“克苏鲁”的反常识作品。
    解决办法是通过大规模数据和暴力计算训练出大模型。
- 2020 年，Open AI 发布 GPT-3 （Generative Pre-trained Transformer 3），是第三代自然语言处理 (NPL) 系统。
  - GPT-3 的模型使用超过 1750 亿个机器学习参数进行训练，是当时最大的神经网络、最先进的文本生成模型。
- 2022 年，流行的技术是扩散模型，源于 2015 年发表的 diffusion 论文。
  - 扩散模型（diffusion model）：逐步添加随机噪声，将样本图像破坏成一张马赛克图像。然后反转这个噪声过程，让 AI 学习如何将马赛克图像恢复成正常的图像。
  优点：
  - 不需要对抗训练。
  - 生成简单图像时比较可靠，比如背景图、纹理，辅助真人画家的工作，相当于 PS 软件的自动填充功能。
  - 很多 AIGC 软件是开源的、免费使用，不会绘画的用户也可以使用，降低了艺术创作的门槛。
    OpenAI 开源的 Dall-E 2 ，可根据用户输入的文字描述，生成图像。
    英国公司 Stability AI 开源的 Stable Diffusion 模型，可根据用户输入的文字描述，在几秒内自动生成图像。
    Novel AI ：一个闭源的网站。在用户输入一些提示词之后，能基于 GPT 模型自动生成小说，还能基于 Stable Diffusion 模型生成图像。
  缺点：
  - 能快速生成大量可读性高的图像，但依然存在小型瑕疵，比如物体破损、人物表情扭曲。通常要尝试生成几十张图像，才能得到一张没有明显瑕疵的图像。
  - 生成的图像只是在少量方面贴近文字描述，难以符合用户期望，不如真人画家。


AIGC 的问题：
- 数据版权。开发人员通常会从互联网获取大量图片用于训练 AI 模型，未经过画家的同意。
- 创作版权。AIGC 的图像通常跟样本图像的元素、画风相似，相当于抄袭原画家，但如何界定是否抄袭？AIGC 的图像应该分配什么样的版权？
- 如果增加 AIGC 的随机性，则生成的内容难以使用，难以代替真人进行艺术创作。因此只能减少 AIGC 的随机性，做一些重复性工作，比如生成一个人物穿多种服装的设定图。
