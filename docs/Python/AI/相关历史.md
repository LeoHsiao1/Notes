# 相关历史

- 1936 年，作为数学学士毕业的 Alan Turing ，在英国剑桥大学担任研究员。他发表论文，设想了一种能执行任何数学算法的通用机器。
  - 这种机器被称为图灵机（Turing machine），工作原理如下：
    - 在一条纸带上，记下一个一个符号。
    - 一台机器，依次读取纸带上的符号，然后根据符号执行数学算法。
    - 机器将数学算法的结果，打印在纸带上。
  - 图灵机是一个理论模型，在数学上证明了通用计算机是可以实现的。
    - 如果一个数学问题可以被表达为有限步骤的数学算法，则可以被图灵机解决。
    - 如果一个编程语言能完成图灵机的工作，比如算术运算、逻辑运算、条件判断、循环，则称为图灵完备（Turing Completeness）。

- 1945 年，参与美国原子弹研制的 John von Neumann 编写了一份论述 EDVAC（Electronic Discrete Variable Automatic Computer）的报告。
  - 他设计了通用计算机的硬件架构，分为五个基本部分：运算器、控制器、存储器、输入设备、输出设备。
  - 这种计算机基本架构，被称为冯诺依曼结构，被绝大部分计算机采用。

- 1950 年， Alan Turing 发表论文《Computing Machinery and Intelligence》
  - 该论文探讨了一个重要问题： Can machines think?
  - 为了解答该问题，他提出了一个模仿游戏，用于判断一个机器能否思考。这个游戏后来被称为图灵测试，原理如下：
    - 安排一个人类 A 与机器 B ，分别与人类 C 对话。
    - 对话双方，采用写信等间接方式，避免看到对方的模样、透露身份信息。
    - 如果对话之后，人类 C 不能分辨与自己聊天的对方，是不是机器。则说明这个机器能模仿人类的思考。

- 1956 年，美国 Dartmouth 学院的教授 John McCarthy 发起一场关于 thinking machines 的研讨会，邀请了 Claude Shannon、Marvin Minsky、Nathaniel Rochester 等学者。
  - 他们探讨了如何让机器使用人类的语言、模仿人类的思考、代替人类解决问题。
  - 他们首次提出了人工智能（Artificial Intelligence）的概念。





2012 年之后，当图形处理单元开始用于加速神经网络并且深度学习优于以前的人工智能技术时，资金和兴趣大幅增加。
随着 2017 年之后的Transformer 架构，这种增长进一步加速。



计算机视觉是深度学习的一种用途，包括目标识别、目标跟踪、图像分割、图像生成等。

## AIGC

AI 内容生成（AI Generated Content，AIGC）技术：泛指使用 AI 自动生成文字、图像、音频、视频、代码等内容，代替人工。


AI 图像生成技术的发展阶段：
- 2015 年，主流的技术是通过基于卷积神经网络的迁移学习，来进行图像风格转换。先对图像内容进行语义分割，再将内容和场景通过线条弯曲、风格迁移等手法，转换成指定艺术风格，类似美颜软件的“滤镜”功能。
- 2016 年，流行的技术是生成对抗网络 GAN（Generative Adversarial Nets）。原理：让生成器网络和判别器网络相互对抗，从而创作出真实度和准确度都更高的全新图像。此时网络上出现了大量“以假乱真”的艺术作品。
  - 但是，GAN也不能摆脱传统AI深度模型的问题：无法理解“逻辑”和“常识”，比如AI能够根据文本关键词把元素堆叠在一起，但因为无法理解隐藏在自然语言背后的逻辑关系，所以经常会画出非常“克苏鲁”的反常识作品。
    解决办法是通过大规模数据和暴力计算训练出大模型。
- 2020 年，Open AI 发布 GPT-3 （Generative Pre-trained Transformer 3），是第三代自然语言处理 (NPL) 系统。
  - GPT-3 的模型使用超过 1750 亿个机器学习参数进行训练，是当时最大的神经网络、最先进的文本生成模型。
- 2022 年，流行的技术是扩散模型，源于 2015 年发表的 diffusion 论文。
  - 扩散模型（diffusion model）：逐步添加随机噪声，将样本图像破坏成一张马赛克图像。然后反转这个噪声过程，让 AI 学习如何将马赛克图像恢复成正常的图像。
  优点：
  - 不需要对抗训练。
  - 生成简单图像时比较可靠，比如背景图、纹理，辅助真人画家的工作，相当于 PS 软件的自动填充功能。
  - 很多 AIGC 软件是开源的、免费使用，不会绘画的用户也可以使用，降低了艺术创作的门槛。
    OpenAI 开源的 Dall-E 2 ，可根据用户输入的文字描述，生成图像。
    英国公司 Stability AI 开源的 Stable Diffusion 模型，可根据用户输入的文字描述，在几秒内自动生成图像。
    Novel AI ：一个闭源的网站。在用户输入一些提示词之后，能基于 GPT 模型自动生成小说，还能基于 Stable Diffusion 模型生成图像。
  缺点：
  - 能快速生成大量可读性高的图像，但依然存在小型瑕疵，比如物体破损、人物表情扭曲。通常要尝试生成几十张图像，才能得到一张没有明显瑕疵的图像。
  - 生成的图像只是在少量方面贴近文字描述，难以符合用户期望，不如真人画家。


AIGC 的问题：
- 数据版权。开发人员通常会从互联网获取大量图片用于训练 AI 模型，未经过画家的同意。
- 创作版权。AIGC 的图像通常跟样本图像的元素、画风相似，相当于抄袭原画家，但如何界定是否抄袭？AIGC 的图像应该分配什么样的版权？
- 如果增加 AIGC 的随机性，则生成的内容难以使用，难以代替真人进行艺术创作。因此只能减少 AIGC 的随机性，做一些重复性工作，比如生成一个人物穿多种服装的设定图。
