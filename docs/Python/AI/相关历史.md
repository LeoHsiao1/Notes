# 相关历史

## 图灵机

- 1936 年，作为数学学士毕业的 Alan Turing ，在英国剑桥大学担任研究员。他发表论文，设想了一种能执行任何数学算法的通用机器。
  - 这种机器被称为图灵机（Turing machine），工作原理如下：
    - 在一条纸带上，记下一个一个符号。
    - 一台机器，依次读取纸带上的符号，然后根据符号执行数学算法。
    - 机器将数学算法的结果，打印在纸带上。
  - 图灵机是一个理论模型，在数学上证明了通用计算机是可以实现的。
    - 如果一个数学问题可以被表达为有限步骤的数学算法，则可以被图灵机解决。
    - 如果一个编程语言能完成图灵机的所有功能，比如算术运算、逻辑运算、条件判断、循环，则称为图灵完备（Turing Completeness）。

- 1945 年，美国的 John von Neumann 在参与研制原子弹的同时，还为 EDVAC（Electronic Discrete Variable Automatic Computer）项目的巨型计算机，设计了一种硬件结构。
  - 他将计算机分为五个基本部分：运算器、控制器、存储器、输入设备、输出设备
  - 这种计算机结构，被称为冯诺依曼结构，后来被大部分计算机采用。

- 1950 年， Alan Turing 发表论文《Computing Machinery and Intelligence》
  - 该论文探讨了一个重要问题： Can machines think?
  - 为了解答该问题，他提出了一个模仿游戏，用于判断一个机器能否思考。这个游戏后来被称为图灵测试，原理如下：
    - 安排一个人类 A 与机器 B ，分别与人类 C 对话。
    - 对话双方，采用写信等间接方式，避免看到对方的模样、透露身份信息。
    - 如果对话之后，人类 C 不能分辨与自己对话的对方，是不是机器。则说明这个机器能模仿人类的思考。

## 人工智能

- 1956 年，美国 Dartmouth 学院的教授 John McCarthy 发起一场关于 thinking machines 的研讨会，邀请了 Claude Shannon、Marvin Minsky、Nathaniel Rochester 等学者。
  - 他们探讨了如何让机器使用人类的语言、模仿人类的思考、代替人类解决问题。
  - 他们首次提出了 AI（Artificial Intelligence，人工智能）的概念，指人类制作的智能。

- 随后几十年，人类研究了许多实现人工智能的技术，主要分为几类：
  - 专家系统
  - 机器学习
  - 符号推理
  - 遗传算法

- AI 技术的常见用途：
  - NLP（Natural Language Processing，自然语言处理）
    - 让 AI 能理解人类的语言、生成文字。
    - 例如将一篇文章，从英语翻译到汉语。
    - 例如分析一篇文章，提取其内容大意。
  - 计算机视觉
    - 让 AI 能理解图像、视频。
    - 例如分析一张图片，识别其中存在什么物体。
    - 例如分析一张图片，找出其中的人脸，并进行人脸识别。
  - 数据预测
    - 让 AI 分析一段数据的历史走势，学习其中的规律，然后预测未来走势。
    - 例如预测股市走势。
    - 例如分析用户的信息，将用户可能感兴趣的广告，推送给用户。
  - AI 决策
    - 让 AI 对某一问题，给出解决方案。
    - 例如分析病人的信息，进行医疗诊断。
  - AIGC（AI Generated Content）
    - 让 AI 生成内容，比如文本、图像、音频、视频。
    - 又称为 Generative AI、GAI 。
    - 优点：
      - 让 AI 代替人类完成一些编辑工作。某些场景下，比人类更快、更好。
    - 缺点：
      - AI 是根据现有资料来生成内容，因此擅长生成重复性内容，不擅长生成新颖内容。虽然增加随机程度，可以生成比较新颖的内容，但是结果更容易混乱、不可用。
      - AI 是根据现有资料来生成内容。如果现有资料的版权属于某个人，那么 AIGC 的内容，版权属于谁？算抄袭吗？这存在法律问题。
  - 机器人控制

- 目前，人类已经研发了许多有用的 AI 技术，能让机器代替人类完成许多任务。但人类尚未做出 AGI（Artificial General Intelligence，人工通用智能）。
  - AGI 的一种定义是： AI 能代替人类完成所有任务，具备人类的所有能力。
  - AGI 的另一种定义是： AI 能自主学习，掌握所有能力，不需要为每种能力单独编程。
  - 哲学上，人们还探讨了 strong AI ：不但拥有智慧、能力，还拥有自我意识、情感。

## 专家系统

- 1965 年，美国斯坦福大学的教授 Edward Feigenbaum 带头研发了专家系统（expert system）。
  - 专家系统是一种基于知识库进行决策的软件。知识库内由人类预设了很多 `If... Then...` 形式的规则。
  - 此时， ANN 技术还在初级阶段，难以投入实际应用。而专家系统的研发难度更低，是当时最热门的人工智能技术。

- 1980 年代，专家系统被人们大规模使用。
  - 比如用于医疗领域：如果病人具有某症状，则给出某个诊断结果。
  - 比如用于法律领域：如果一件事符合某法律条文，则给出某个法律结论。
  - 比如一些大学，开设了专家系统的课程。
  - 人们对于专家系统过于乐观，希望它能代替人类进行决策。但随着推广使用，专家系统的缺陷越来越明显：
    - 只能基于呆板的规则进行决策，难以处理意外的情况。
    - 只能进行简单的规则判断，一旦规则变得复杂，就容易出错。

- 1990 年代，研究专家系统的学者越来越少，毕竟 ANN 技术的能力更强、潜力更大。
  - 有些软件依然采用专家系统进行决策，毕竟研发难度比 ANN 技术低很多。
  - 有些软件将专家系统换了个称呼，改名为规则引擎。

## 机器学习

- 1956 年，美国 IBM 公司的 Arthur Samuel ，在 IBM 的第一台商用计算机上，编写了一个能学习下棋的跳棋程序。
  - 当时，他的跳棋程序，已经能战胜业余的人类棋手。这引发了公众轰动，毕竟商用计算机才出现没几年，人们惊叹于计算机的学习能力。
  - Arthur Samuel 写了一个论文，解释这个跳棋程序的原理：
    - 如何让计算机学习下棋？最直接的方案，是让计算机穷举所有可能的棋局，从中找出通往胜利的下棋位置。
    - 但当时的计算机内存很少，因此他采取的方案是：编写一个函数，分析每个下棋位置的获胜概率，从中找出胜率最大的下棋位置。
    - 他还改进了跳棋程序，让程序能记住已发生的棋局位置，保留经验。
    - 他还让程序自己与自己对弈几千场棋，从而积累经验。
  - Arthur Samuel 在论文中，首创了机器学习（Machine Learning）这一概念。
    - 他对机器学习的定义是：使得计算机无需明确编程，就能学习解决问题。
    - 后来，人们改进了机器学习的定义：计算机执行某种任务时，随着经验的增加，能提升某种性能指标。

- 1987 年，美国卡内基梅隆大学的博士生徐峰雄，为国际象棋研发了一台名为 ChipTest 的超级计算机，赢得了北美计算机国际象棋锦标赛的冠军。
  - 徐峰雄毕业之后，加入了 IBM 公司，带队研发了一台名为 Deep Blue 的超级计算机，继续参加国际象棋比赛。
- 1997 年， Deep Blue 在国际象棋比赛中，以2胜1负3平的成绩，战胜了人类的世界冠军 Garry Kasparov 。
  - 这标志着计算机已经能在复杂棋类游戏中战胜人类，引发了公众轰动。
  - Deep Blue 没有采用神经网络，而是由人类手动编写复杂算法、预设大量决策规则。
    - 因此 Deep Blue 能勉强胜任国际象棋，但不能胜任围棋，因为围棋的棋局可能性多很多倍。

- 人类为实现机器学习这一目的，研发了许多种算法，可分为几类：
  - 监督学习
    - 给程序提供一组输入、输出作为示例数据，让程序学习从这些输入映射到输出的一般规律。
    - 通常，需要给程序提供大量数据，让程序找出统计学上的规律。
  - 无监督学习
    - 不提供示例数据，让程序直接学习规律。
  - 半监督学习
    - 只提供少量示例数据。
  - 强化学习
    - 让程序执行某种任务，如果表现好，就给予正反馈作为奖励，从而鼓励程序表现得越来越好。

### 神经网络

- 1943 年，美国的神经科学家 Warren McCulloch 与 Walter Pitts 发表论文，分析了人脑神经元的工作逻辑。
  - 人类大脑的思维过程，是由许多神经元协同工作。每个神经元负责完成一种简单逻辑：如果满足某条件，则做出某决策。
    - 例如，对于 "今天逛公园吗" 这个问题，需要考虑多个条件：
      - 如果喜欢逛公园，则提高意愿一定程度。
      - 如果天气下雨，则降低意愿一定程度。
      - 如果身体不适，则降低意愿一定程度。
  - 可以为一个神经元，建立一个数学模型：
    - 定义多个输入变量，用一个矢量 `X = (x1, x2, x3, ...)` 表示。
    - 每个输入变量，存在不同的权重，用一个矢量 `Y = (y1, y2, y3, ...)` 表示。
    - 定义一个阈值 threshold ，它决定了输出变量 output 的取值：
      - 如果 `X*Y ≤ threshold` ，则 `output = 0` ，决策为否
      - 如果 `X*Y > threshold` ，则 `output = 1` ，决策为是。
    - 只要给矢量 Y 、阈值 threshold 分配合适的值，就能让这个数学模型，模拟人类的决策逻辑。
  - 受此启发，他们提出了一种软件方案：
    - 编写许多个软件单元，每个单元像一个神经元，负责处理一种逻辑判断。
    - 这些软件单元可以协同工作，模拟人脑的神经网络。
    - 只要有足够多的软件单元，这种软件可以执行任何数学算法，实现图灵机。

- 后来，人们继续研究这种模拟神经网络的软件方案，称为人工神经网络（Artificial Neural Network，ANN），简称为 NN 。
  - 编程实现神经网络时，主要逻辑如下：
    - 在代码中存储了权重矢量 Y 、阈值 threshold 的取值。
    - 给代码输入矢量 X 。
    - 代码会自动输出变量 output 。
  - 权重矢量 Y 、阈值 threshold 的取值，是算法模型中的核心参数。如何确定它们的取值？
    - 最初，人们手动赋值，然后测试效果。如果效果不好，就换个取值再测试。
    - 后来，人们编写程序来自动赋值、自动测试。这个过程称为训练模型。
  - 可以将一个神经单元的输出，用作另一个神经单元的输入。以此类推，可以组成 n 层神经网络，实现复杂的决策逻辑。
    - 多层神经网络，还可以采用环形结构，让信号循环传递到各个神经元。这种结构称为 RNN（Recurrent Neural Network）。
    - 用多层神经网络来实现机器学习的方案，称为深度学习（Deep Learning）。

- 1958 年，美国的心理学家 Frank Rosenblatt 发明了感知器（Perceptron）。
  - 它是一个三层的神经网络，包含输入层、隐含层、输出层。
  - 它能检查输出值，如果与预期值不一致，则自动调整权重参数。
  - 它是世界上第一个能自动训练的 ANN 模型，能自己学习，不需要人类手动分配参数。

- 1965 年，苏联的数学家 Alexey Ivakhnenko 发明了 GMDH（Group Method of Data Handling） 算法。
  - 它是第一个可行的深度学习算法，能训练任意 n 层神经网络。
  - 它能根据经验数据，自动确定模型的结构和参数，实现归纳学习。

- 1969 年，日本的福岛邦彦发明了 ReLU（Rectified Linear Unit，整流线性单元），它成为了深度学习中常用的激活函数。
- 1979 年，福岛邦彦发明了第一个带有卷积层的神经网络，开创了卷积神经网络（Convolutional Neural Network，CNN）这一架构。
  - 福岛邦彦的研究领域是图像识别，他从人脑的视觉细胞中得到了启发：存在一种处理简单任务的细胞、一种处理复杂任务的细胞，两种细胞级联工作。

- 1982 年，美国的 Paul Werbos 使用误差反向传播算法，成功训练了多层神经网络。
  - 感知器等更早的算法，只能训练单层神经网络。如果训练多层神经网络，则不能有效地调整多个层的权重。
  - 反向传播算法，能计算每一层的梯度（误差），并将误差从输出层逐层传到输入层，从而逐层调整权重，因此能训练多层神经网络。

- 1989 年，美国贝尔实验室的 Yann LeCun 等人，创建了一个名为 LeNet 的 CNN ，用于识别邮件上的手写邮编。

- 1991 年，德国的 Sepp Hochreiter 在毕业论文中，发现了梯度消失问题。
  - 当时，反向传播算法虽然可以训练多层神经网络，但效果并不好。他找到了问题的原因：梯度传播多层时，较早权重的梯度，会在运算中被缩小幅度。

- 1995 年， Sepp Hochreiter 发明了 LSTM（Long Short Term Memory，长短期记忆）技术，可以解决 RNN 网络的梯度消失问题。
  - LSTM 可以为 RNN 提供可持续数千个时间步的记忆。

- 2012 年，加拿大的 Alex Krizhevsky 及其博士生导师 Geoffrey Hinton ，研发了一个 CNN 网络，名为 AlexNet ，在 ImageNet 图像分类比赛中取得了显著优势。
  - 他们使用两张 Nvidia GPU 来训练模型，比起使用 CPU 来训练模型，耗时大幅缩短。
  - 这引发越来越多人，使用 GPU 来加速训练模型。

- 2016 年，英国 DeepMind 公司被 Google 收购，研发了 AlphaGo 程序，在围棋比赛中，战胜了人类的世界冠军李世石。
  - AlphaGo 采用深度学习，因此不必像 Deep Blue 那样由人类手动编写大量决策规则。
  - AlphaGo 进行了大量训练，包括与人类对弈、与计算机对弈，从而逐渐提高了下棋实力。

## NLP


### Transformer

- 2011 年， Apple 公司发布一款名为 Siri 的对话机器人，安装在 iPhone 等电子产品中。
  - 此时 iPhone 这种触屏手机刚发布 4 年， Siri 实在是一个新潮的技术，是许多人首次接触的对话机器人。
  - 此时 AI 技术跟 20 世纪差不多，只是互联网技术大幅发展，使得 Siri 可以联网搜索资料。
  - Siri 的缺点很多：
    - 它的语音识别能力差，如果用户说话不够清晰、带口音，它就不能识别为文本。
    - 它的语言处理能力差，即使识别了用户说的一句话是什么，也经常误解用户的意图。
    - 它几乎没有逻辑推理能力，只能僵硬地执行用户的命令，比如拨打电话、地图导航。

- 2014 年，微软公司发布一款名为小冰的对话机器人，以网页形式供用户访问。
  - 它基于深度学习的 RNN + LSTM 技术，使用大规模语料库进行训练，能与人类进行日常对话。但不能进行长对话。
  - 它能记住用户习惯等上下文信息，据此调整回答。但只能记住少量上下文信息。
  - 它能分析用户说话的语气、情感，据此调整回答。但只能进行初步的分析。

- 2017 年， Google 公司的几位研究员发布论文《Attention is all you need》，提出一种新的深度学习架构，名为 Transformer 。
  - 当时，对于序列类型的数据（比如文本、音频），业界通常采用 RNN + LSTM 技术来处理。
    - 缺点：能记住一定长度的上下文信息，但超过长度就记不住，因此不能处理较长的序列。
    - 缺点：需要按顺序读取一个序列中的每个 token ，耗时久。
  - 谷歌翻译的 seq2seq 模型也采用 RNN + LSTM 技术。为了解决上述缺点，研究员做了以下改进：
    - 引入注意力机制，代替 LSTM ，使得模型不必记住全部上下文信息，只需记住有用部分。因此模型可以处理较长的序列。
    - 移除循环，并行化处理 token ，从而能在 GPU 上加速执行。

- 2018 年， Google 公司发布一个名为 BERT 的语言模型。
  - 它采用 Transformer 架构。
  - 它进行了生成式预训练（Generative Pre-training，GP），这是一种半监督学习：
    - 先用一个未标记的、所有领域的数据集（比如 Wikipedia 等互联网资料），训练模型的通用能力。
    - 再用一个标记的、单个领域的数据集，进一步微调模型在该领域的能力。
  - 它使用两种方法来预训练：
    - 掩码标记预测：将句子中随机几个单词掩盖，让模型猜测这几个单词是什么。
    - 下一句预测：输入两个句子，比如 "I have a car." 与 "It is small." ，让模型判断这两个句子在语意上是否连续。
  - 传统的语言模型，只能按单个方向分析一个序列中的每个 token 。而 BERT 能顺序、倒序同时分析序列，建立两个方向的上下文。

### GPT

- 2018 年，美国的初创公司 OpenAI 发布一个名为 GPT（Generative Pre-trained Transformer）的语言模型。
  - 它采用 Transformer 架构，进行了预训练+监督微调。
  - 它的模型参数有 1 亿个。
  - 它主要用于生成文本，但也可以生成图像、音频，只是没有经过针对性训练。
  - 缺点：它生成的文本依然比较呆板，容易出现逻辑错误、内容重复。

- 2019 年， OpenAI 公司发布 GPT 的 v2 版本。
  - 与上一代 GPT 相比，它的训练数据集大了几倍，模型参数增加到 15 亿个。
  - 缺点：它生成的文本比较流畅，但依然容易逻辑错误、忘记上下文信息。

- 2020 年， OpenAI 公司发布 GPT 的 v3 版本。
  - 与上一代 GPT 相比，它的训练数据集大了几倍，模型参数增加到 1750 亿个。
  - 它生成的文本很流畅，能处理复杂的长文本，偶尔才出现逻辑错误。换句话说，它处理自然语言的能力逼近真人，在许多情况下可以通过图灵测试。
  - 不过，它对现实世界的理解容易出错：
    - 回答用户的问题时，可能编造不存在的历史事实。这个问题，被人们称为模型的幻觉。
    - 在用户的诱导下，可能说出明显违背事实的错误言论。

- 2022 年 11 月 30 日， OpenAI 公司发布一款名为 ChatGPT 的对话机器人。
  - ChatGPT 提供了网页版对话框，供用户使用。也提供了 API 。
  - ChatGPT 最初采用 GPT-3.5 模型，后来用户可选用 GPT-4 等其它模型。
  - ChatGPT 发布之后，迅速火爆世界， 2 个月时间就拥有 1 亿用户。公众惊讶 AI 技术已经这么强大。
    - 之前的 Siri 等对话机器人，只能进行简单的日常对话。但 ChatGPT 能进行复杂的长对话，几乎像人类。
    - ChatGPT 能理解文本中的语气、情感，甚至能讲幽默笑话。
    - 用户可以问任何领域的专业问题， ChatGPT 都能给出解答（因为用 Wikipedia 等数据训练了模型）。它像一个什么都懂的网友，只是回答不一定完全正确。
    - 许多学生，用 ChatGPT 解答自己的学校作业、考试试卷。
    - 在编程这个专业领域， ChatGPT 能理解一段代码的功能逻辑，进行讲解。能根据用户的需求，生成一段源代码。（因为用 Stack Overflow 等数据训练了模型）
  - ChatGPT 也让许多 AI 学者感到惊讶。
    - 在此之前，对话机器人虽然能与人类日常对话，但不能理解人类世界的常识、不能进行逻辑推理。
    - 因此，许多学者认为，等研究出 AGI 之后，才能让 AI 掌握常识、逻辑。
    - 但 ChatGPT 证明，如果模型训练了海量参数，就能记住很多常识、逻辑，能在许多场合模拟人类的智慧。这种模型被称为 LLM（Large Language Model）。

- 2023 年， OpenAI 公司发布 GPT 的 v4 版本。
  - 它的模型参数超过 1 万亿个。
  - 它增强了多模态能力，能处理多种类型的数据（比如文字生成、图像生成、音频生成），不必为每种数据分别训练一个模型。
  - 它对涉及安全、道德的的内容，进行了更多的对抗训练、内容审查，使得模型很少生成带偏见、有害的内容。

- 2025 年， OpenAI 公司发布 GPT 的 v5 版本。
  - 它带有一个路由器，会根据用户的提问难度，自动选择 GPT-5-main、GPT-5-thinking、GPT-5-mini 等模型来回答。

### LLM

- ChatGPT 火爆世界之后，证明了 LLM 的巨大潜力。许多公司急忙跟着研发 LLM 模型，进行技术竞赛。

- Anthropic 公司
  - 2021 年， OpenAI 的几名员工离职，创立了 Anthropic 公司。
  - 2023 年 3 月，发布一款名为 Claude 的模型。这个名字是致敬信息论的奠基人 Claude Shannon 。

- 微软公司
  - 2019 年，向 OpenAI 公司投资 10 亿美元，约定将 OpenAI 公司的模型部署到 Azure 云平台。
  - 2020 年，为 OpenAI 公司定制了一台超级计算机，拥有 1 万张显卡。这提高了 OpenAI 训练模型的速度。
  - 2023 年，追加 100 亿美元投资，获得 OpenAI 的 49% 股权。将 GPT-3 的 API 整合到微软公司的产品中，命名为 Copilot 。
  - 2024 年，与 OpenAI 公司的合作减少，开始自研模型。
  - 2025 年，发布自研模型 MAI 。

- Google 公司
  - 2020 年，发布一款名为 Meena 的对话机器人，模型参数有 26 亿个。
    - Google 公司限制了 Meena 被公众使用，因为担心对话机器人的能力太强，可能被用户滥用，比如网络诈骗。
  - 2021 年，将 Meena 的模型改名为 LaMDA （Language Model for Dialogue Applications），模型参数有 1370 亿个。
  - 2022 年，发布 LaMDA 的 v2 版本。
  - 2023 年 2 月，发布一款名为 Bard 的对话机器人，基于 LaMDA 模型。
    - Bard 是匆忙发布的产品，在发布会上展示的效果较差，导致 Google 公司的股价下跌 10% 。
  - 2023 年 12 月，发布一款名为 Gemini 的模型，它继承 LaMDA 但做了大幅修改。

- Meta 公司
  - 2023 年 2 月，发布一款名为 Llama 的模型。

- xAI 公司
  - 2023 年 3 月，马斯克创立 xAI 公司，加入 LLM 赛道。
  - 2023 年 11 月，发布名为 Grok 的自研模型，并以对话机器人的形式，集成到 X 网站上，供网络用户免费使用。

- DeepSeek
  - 2016 年，梁文锋创立 High-Flyer 公司，利用 AI 技术进行量化交易。
  - 2023 年， High-Flyer 公司将 AI 部门拆分为独立公司 DeepSeek 。
  - 2023 年 11 月，发布一款名为 DeepSeek-Coder 的模型，其能力较差，还在追赶 ChatGPT 的技术。
  - 2024 年，先后发布 DeepSeek-V2 和 DeepSeek-V3 。
  - 2025 年 1 月，发布 DeepSeek-R1 模型，能力接近 GPT-4 。
    - 它在预训练之后，没有监督微调，而是单纯进行强化学习，从而提升模型的推理能力。
    - 它利用知识蒸馏技术，将大型模型的知识、推理能力，传递给小型模型。
    - 它在网络上公开了模型文件，用户可以下载之后自己部署，而且部署成本低。
      - 最小的模型只有 15 亿参数，简称为 1.5B 。在 8G 内存的个人电脑上就可以部署，甚至可以用 CPU 运行，不必用 GPU 。
      - 大模型有 70B 参数，部署时需要几张显卡、几百GB内存。
    - 它以网页形式，提供了一个免费的 DeepSeek 对话机器人，吸引了许多中文用户。
      - 它支持在网页上显示模型的推理过程，方便用户理解模型的思维方式。

## AIGC


- 21 世纪初， AIGC 技术还在起步阶段，只能在少量场景，执行简单任务。
  - 例如进行图像风格的转换：基于 CNN 技术，先对图像内容进行语义分割，再通过线条弯曲、风格迁移等手法，将图片内容转换成另一种艺术风格，实现修图软件的滤镜效果。

- 2014 年， Google 公司的 Ian Goodfellow 发明了 GAN（Generative Adversarial Network，生成对抗网络）技术。
  - 这是使用两个神经网络来对抗。一个神经网络负责生成图像，另一个神经网络负责检测该图像是真是假。
  - 多次重复该过程，两个神经网络都会不断学习进步，从而提高生成的图像质量。

- 2022 年，流行的技术是扩散模型，源于 2015 年发表的 diffusion 论文。
  - 扩散模型（diffusion model）：逐步添加随机噪声，将样本图像破坏成一张马赛克图像。然后反转这个噪声过程，让 AI 学习如何将马赛克图像恢复成正常的图像。
  优点：
  - 不需要对抗训练。
  - 生成简单图像时比较可靠，比如背景图、纹理，辅助真人画家的工作，相当于 PS 软件的自动填充功能。
  - 很多 AIGC 软件是开源的、免费使用，不会绘画的用户也可以使用，降低了艺术创作的门槛。
    OpenAI 开源的 Dall-E 2 ，可根据用户输入的文字描述，生成图像。
    英国公司 Stability AI 开源的 Stable Diffusion 模型，可根据用户输入的文字描述，在几秒内自动生成图像。
    Novel AI ：一个闭源的网站。在用户输入一些提示词之后，能基于 GPT 模型自动生成小说，还能基于 Stable Diffusion 模型生成图像。
  缺点：
  - 能快速生成大量可读性高的图像，但依然存在小型瑕疵，比如物体破损、人物表情扭曲。通常要尝试生成几十张图像，才能得到一张没有明显瑕疵的图像。
  - 生成的图像只是在少量方面贴近文字描述，难以符合用户期望，不如真人画家。

与 LLM 模型结合
