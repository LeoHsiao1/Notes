# Web 爬虫

Web 服务器收到的 HTTP 请求一般都来自真人用户操纵的 Web 浏览器，有时也可能来自 Web 爬虫等软件。

## 爬虫

：网络爬虫（web crawler），又称为网络蜘蛛（web spider），是一些自动从 Web 网站获取信息的程序。
- 通常用于自动向 Web 网站发出 HTTP 请求，并从 HTTP 响应报文中提取出有用的数据。
- Google 等搜索引擎就运行了大量爬虫，爬取大量网站的内容，供用户搜索。
- 很多种编程语言都可用于开发 Web 爬虫。其中，使用 Python 开发爬虫尤其方便。
- 优点：
  - 通常比人工收集数据的效率高很多。
- 缺点：
  - 爬虫可能被用于非法获取网站的私密数据，还可能消耗网站的大量带宽、增加负载。因此，很多网站会采取一些反制爬虫的措施，比如加验证码。

## 无头浏览器

：Headless Browser ，指不显示 GUI 界面的 Web 浏览器，需要通过 API 控制。
- 优点：
  - 不提供 GUI 界面，减少了开销，可在没有显卡的服务器上运行。
  - 常用于爬虫、Web 自动化测试、网页截图。
- 有时，爬虫不能直接从网站的 HTTP 响应报文中提取有用的数据，还需要执行 JS 代码，此任务就适合调用无头浏览器来完成。
  - 例如网页的 HTML body 原本为空，需要经过 AJAX 动态加载，才会生成有效内容。
- 常见的几种无头浏览器：
  - [PhantomJS](https://github.com/ariya/phantomjs) ：于 2011 年发布，2018 年暂停开发。
  - Chrome ：支持 Headless 模式。
  - Firefox ：支持 Headless 模式。

## robots.txt

：放在网站根目录下的一个文本文件，用于声明该网站的哪些资源允许被搜索引擎、爬虫访问。
- 它只是一种声明，不具有强制的约束力。
  - 当搜索引擎、爬虫访问一个网站时，应该先检查该网站的根目录下是否存在 robots.txt 文件。如果存在，则遵守其要求，限制访问范围；如果不存在，则默认可以访问该网站的所有资源。
- 内容示例：
  ```sh
  User-agent: *           # 声明该规则作用于哪些 robot ，可以使用通配符
  Disallow: /js/*         # 禁止访问指定路径的文件
  Allow: /img/            # 允许访问指定路径的文件

  Sitemap: https://test.com/sitemap.xml
  ```

## sitemap.xml

：放在网站根目录下的一个文本文件，以 XML 格式存储了该网站的所有链接，作为网站地图，方便被搜索引擎爬取、收录。
- 也可以放在网站其它路径，然后在 robots.txt 中声明。

## SEO

：搜索引擎优化（Search Engine Optimization），指通过多种方式提高网站在搜索引擎的排名，从而提高网站的曝光量、访问量。

### ♢ scrapy

：Python 的第三方库，是一个爬虫框架，支持异步处理多个并发请求。
